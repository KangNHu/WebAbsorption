package com.liveeasy.web.absorption.core;import com.liveeasy.web.absorption.core.annotation.SPI;import com.liveeasy.web.absorption.core.factory.WebAbsorptionComponentCloneBeanFactory;import com.liveeasy.web.absorption.core.pipeline.CentralPipeline;import com.liveeasy.web.absorption.core.pipeline.DataStreamPipeline;import com.liveeasy.web.absorption.core.pipeline.WebAbsorptionPipeline;import com.liveeasy.web.absorption.core.plugin.CentralDownloaderPlugin;import com.liveeasy.web.absorption.core.plugin.CentralProxyProvider;import com.liveeasy.web.absorption.core.plugin.DownloaderPlugin;import com.liveeasy.web.absorption.core.processor.CentralProcessor;import com.liveeasy.web.absorption.core.processor.WebAbsorptionProcess;import com.liveeasy.web.absorption.core.schehule.DataResultItemsSchedule;import com.liveeasy.web.absorption.core.schehule.DataStreamSchedule;import com.liveeasy.web.absorption.core.schehule.QueueDataResultItemsSchedule;import com.liveeasy.web.absorption.core.schehule.QueueDataStreamSchedule;import org.apache.commons.lang3.StringUtils;import us.codecraft.webmagic.Site;import us.codecraft.webmagic.Spider;import us.codecraft.webmagic.SpiderListener;import us.codecraft.webmagic.scheduler.Scheduler;import java.util.*;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.ExecutorService;import java.util.stream.Collectors;/** * WebAbsorption类 */public class WebAbsorption {    private Spider spider;    private Site site;    private String spiderName;    private boolean isAsyncPipeline = false;    private int threadNum = 1;    private Scheduler scheduler;    private DataResultItemsSchedule dataResultItemsSchedule = new QueueDataResultItemsSchedule();    private DataStreamSchedule dataStreamSchedule = new QueueDataStreamSchedule();    private ExecutorService executorService;    private DownloaderPlugin downloaderPlugin;    private DataStreamPipeline dataStreamPipeline;    private List<WebAbsorptionPipeline> webAbsorptionPipelines;    private List<WebAbsorptionProcess> webAbsorptionProcesses;    private String id;    private static WebAbsorptionComponentCloneBeanFactory factory = WebAbsorptionComponentCloneBeanFactory.newInstance();    private static Map<String , DataStreamPipeline> streamPipelineMap = new ConcurrentHashMap<>();    private static Map<String , List<WebAbsorptionPipeline>> pipelineMap = new ConcurrentHashMap<>();    private static Map<String , List<WebAbsorptionProcess>> processMap = new ConcurrentHashMap<>();    private WebAbsorption(){};    static {        ServiceLoader<WebAbsorptionPipeline> webAbsorptionPipelines = ServiceLoader.load(WebAbsorptionPipeline.class);        ServiceLoader<WebAbsorptionProcess> webAbsorptionProcesses = ServiceLoader.load(WebAbsorptionProcess.class);        for (Iterator<WebAbsorptionPipeline> iterator = webAbsorptionPipelines.iterator(); iterator.hasNext() ;){            WebAbsorptionPipeline webAbsorptionPipeline = iterator.next();            Class<? extends WebAbsorptionPipeline> pClass = webAbsorptionPipeline.getClass();            if(!pClass.isAnnotationPresent(SPI.class)){                continue;            }            SPI spi = pClass.getAnnotation(SPI.class);            if(webAbsorptionPipeline.getType() == WebAbsorptionPipeline.DATA_STREAM_PIPELINE){                streamPipelineMap.putIfAbsent(spi.value() , (DataStreamPipeline) webAbsorptionPipeline);            }else {                List<WebAbsorptionPipeline> pipelineList = pipelineMap.get(spi.value());                if(pipelineList == null){                    pipelineList = new LinkedList<>();                    pipelineMap.put(spi.value() , pipelineList);                }                pipelineList.add(webAbsorptionPipeline);            }        }        for (Iterator<WebAbsorptionProcess> iterator = webAbsorptionProcesses.iterator() ; iterator.hasNext() ;){            WebAbsorptionProcess webAbsorptionProcess = iterator.next();            Class<? extends WebAbsorptionProcess> pClass = webAbsorptionProcess.getClass();            if(!pClass.isAnnotationPresent(SPI.class)){                continue;            }            SPI spi = pClass.getAnnotation(SPI.class);            List<WebAbsorptionProcess> processes = processMap.get(spi.value());            if(null == processes){                processes = new LinkedList<>();                processMap.put(spi.value() , processes);            }            processes.add(webAbsorptionProcess);        }    }    /**     * 获取WebAbsorption实例     * @return     */    public static WebAbsorption newInstance(){        return new WebAbsorption();    }    /**     * 设置数据流处理器 根据具体情况考虑是否设置     * @param dataResultItemsSchedule     * @return     */    public WebAbsorption setDataResultItemsSchedule(DataResultItemsSchedule dataResultItemsSchedule){        this.dataResultItemsSchedule = dataResultItemsSchedule;        return this;    }    /**     * 设置管道的异步或者同步默认为同步     * @param isAsyncPipeline     * @return     */    public WebAbsorption isAsyncPipeline(boolean isAsyncPipeline){        this.isAsyncPipeline = isAsyncPipeline;        return this;    }    /**     * 设置爬虫的线程数 默认线程数为1     * @param num     * @return     */    public WebAbsorption setThreadNum(int num){        this.threadNum = num;        return this;    }    /**     * 设置爬虫的线程池如果不设置则使用默认线程池     * @param executorService     * @return     */    public WebAbsorption setExecutorService(ExecutorService executorService){        this.executorService = executorService;        return this;    }    /**     * 设置爬虫ID     * @param id     * @return     */    public WebAbsorption setID(String id){        this.id = id;        return this;    }    /**     * 设置下载插件 根据具体情况考虑是否设置     * @param downloaderPlugin     * @return     */    public WebAbsorption setDownloaderPlugin(DownloaderPlugin downloaderPlugin){        this.downloaderPlugin = downloaderPlugin;        return this;    }    /**     * 设置基本爬取参数如果不设置则采用默认设置     * @param site     * @return     */    public WebAbsorption setSite(Site site){        this.site = site;        return this;    }    /**     * 设置数据流处理器，默认使用{@link QueueDataStreamSchedule}     * @param dataStreamSchedule     * @return     */    public WebAbsorption setDataStreamSchedule(DataStreamSchedule dataStreamSchedule){        this.dataStreamSchedule = dataStreamSchedule;        return this;    }    /**     * 设置url请求队列处理器 ，默认使用{@link us.codecraft.webmagic.scheduler.QueueScheduler}     * @param scheduler     * @return     */    public WebAbsorption setScheduler(Scheduler scheduler){        this.scheduler = scheduler;        return this;    }    /**     * 获取Webmagic爬虫对象     * @return     */    public Spider getSpider(){        return this.spider;    }    /**     * 获取爬虫名     * @return     */    public String getSpiderName(){        return this.spiderName;    }    /**     * 建造WebAbsorption     * @param spiderName     * @param startUrls     * @return     * @throws IllegalAccessException     */    public WebAbsorption builder(String spiderName , String... startUrls) throws IllegalAccessException {        if(site == null){            site = Site.me().setCycleRetryTimes(3).setSleepTime(100).setTimeOut(10000);        }        this.spiderName = spiderName;        List<WebAbsorptionPipeline> pipelines = pipelineMap.get(spiderName);        List<WebAbsorptionProcess> processes = processMap.get(spiderName);        DataStreamPipeline dataStreamPipeline = streamPipelineMap.get(spiderName);        if(pipelines == null || processes == null){            throw new IllegalAccessException("没有定义pipeline 和 process");        }        this.dataStreamPipeline = factory.getClone(dataStreamPipeline);        this.webAbsorptionPipelines = factory.getCloneList(pipelines);        this.webAbsorptionProcesses = factory.getCloneList(processes);        dataStreamSchedule.setDataStreamPipeline(this.dataStreamPipeline);        dataResultItemsSchedule.setWebAbsorptionPipelines(this.webAbsorptionPipelines);        CentralPipeline centralPipeline = factory.getClone(CentralPipeline.class)                .setDataStreamSchedule(dataStreamSchedule)                .setDataResultItemsSchedule(dataResultItemsSchedule)                .setAsync(isAsyncPipeline);        CentralProcessor centralProcessor = factory.getClone(CentralProcessor.class)                .setSite(site)                .setWebAbsorptionProcess(this.webAbsorptionProcesses);        CentralProxyProvider centralProxyProvider = null;        CentralDownloaderPlugin centralDownloaderPlugin = factory.getClone(CentralDownloaderPlugin.class);        List<SpiderListener> listeners = new ArrayList<>();        if(null != downloaderPlugin) {            centralProxyProvider = factory.getClone(CentralProxyProvider.class).setDownloaderPlugin(downloaderPlugin);            centralDownloaderPlugin.setDownloaderPlugin(downloaderPlugin);            dataResultItemsSchedule.setDownloaderPlugin(downloaderPlugin);            dataStreamSchedule.setDownloaderPlugin(downloaderPlugin);            listeners.add(downloaderPlugin);        }        centralDownloaderPlugin.setProxyProvider(centralProxyProvider);        this.spider = Spider.create(centralProcessor).addPipeline(centralPipeline).setExecutorService(executorService).setDownloader(centralDownloaderPlugin);        spider.setSpiderListeners(listeners);        if(!StringUtils.isEmpty(this.id)){            spider.setUUID(id);        }        if(threadNum > 0){            spider.thread(threadNum);        }        if(null != scheduler){            spider.setScheduler(scheduler);        }        spider.addUrl(startUrls);        return this;    }    /**     *     * @param isAsync     * @throws IllegalAccessException     */    private void start(boolean isAsync) throws IllegalAccessException {        DataStreamPipeline dataStreamPipeline = dataStreamSchedule.getDataStreamPipeline();        if(this.spider == null){            throw new IllegalAccessException("未调用builder方法进行建造");        }        if(null != dataStreamPipeline){            dataStreamSchedule.start();        }        if(null != dataResultItemsSchedule){            if(isAsyncPipeline){                dataResultItemsSchedule.start();            }        }        if(isAsync) {            spider.runAsync();        }else {            spider.run();        }    }    /**     * 同步启动爬虫     * @throws IllegalAccessException     */    public void run() throws IllegalAccessException {        start( false);    }    /**     * 获取数据流管道     * @return     */    public DataStreamPipeline getDataStreamPipeline() {        return dataStreamPipeline;    }    /**     * 获取结果集管道集     * @return     */    public List<WebAbsorptionPipeline> getWebAbsorptionPipelines() {        return webAbsorptionPipelines;    }    /**     * 获取页面处理器     * @return     */    public List<WebAbsorptionProcess> getWebAbsorptionProcesses() {        return webAbsorptionProcesses;    }    /**     * 判断爬虫是否关闭     * @return     * @throws IllegalAccessException     */    public boolean isClose() throws IllegalAccessException {        if(null == spider){            throw new IllegalAccessException("未调用builder方法进行建造");        }        return spider.isExitWhenComplete();    }    /**     * 异步启动     * @throws IllegalAccessException     */    public void runAsync() throws IllegalAccessException {        start(true);    }}