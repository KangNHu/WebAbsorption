package com.liveeasy.web.absorption.core;import com.liveeasy.web.absorption.core.annotation.SPI;import com.liveeasy.web.absorption.core.factory.WebAbsorptionComponentCloneBeanFactory;import com.liveeasy.web.absorption.core.pipeline.CentralPipeline;import com.liveeasy.web.absorption.core.pipeline.DataStreamPipeline;import com.liveeasy.web.absorption.core.pipeline.WebAbsorptionPipeline;import com.liveeasy.web.absorption.core.plugin.CentralDownloaderPlugin;import com.liveeasy.web.absorption.core.plugin.CentralProxyProvider;import com.liveeasy.web.absorption.core.plugin.DownloaderPlugin;import com.liveeasy.web.absorption.core.processor.CentralProcessor;import com.liveeasy.web.absorption.core.processor.WebAbsorptionProcess;import com.liveeasy.web.absorption.core.schehule.DataResultItemsSchedule;import com.liveeasy.web.absorption.core.schehule.DataStreamSchedule;import com.liveeasy.web.absorption.core.schehule.QueueDataResultItemsSchedule;import com.liveeasy.web.absorption.core.schehule.QueueDataStreamSchedule;import org.apache.commons.lang3.StringUtils;import us.codecraft.webmagic.Site;import us.codecraft.webmagic.Spider;import us.codecraft.webmagic.scheduler.Scheduler;import java.util.*;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.ExecutorService;import java.util.stream.Collectors;public class WebAbsorption {    private Spider spider;    private Site site;    private String spiderName;    private boolean isAsyncPipeline = false;    private int threadNum = 1;    private Scheduler scheduler;    private DataResultItemsSchedule dataResultItemsSchedule = new QueueDataResultItemsSchedule();    private DataStreamSchedule dataStreamSchedule = new QueueDataStreamSchedule();    private ExecutorService executorService;    private DownloaderPlugin downloaderPlugin;    private String id;    private static WebAbsorptionComponentCloneBeanFactory factory = WebAbsorptionComponentCloneBeanFactory.newInstance();    private static Map<String , DataStreamPipeline> streamPipelineMap = new ConcurrentHashMap<>();    private static Map<String , List<WebAbsorptionPipeline>> pipelineMap = new ConcurrentHashMap<>();    private static Map<String , List<WebAbsorptionProcess>> processMap = new ConcurrentHashMap<>();    private WebAbsorption(){};    static {        ServiceLoader<WebAbsorptionPipeline> webAbsorptionPipelines = ServiceLoader.load(WebAbsorptionPipeline.class);        ServiceLoader<WebAbsorptionProcess> webAbsorptionProcesses = ServiceLoader.load(WebAbsorptionProcess.class);        for (Iterator<WebAbsorptionPipeline> iterator = webAbsorptionPipelines.iterator(); iterator.hasNext() ;){            WebAbsorptionPipeline webAbsorptionPipeline = iterator.next();            Class<? extends WebAbsorptionPipeline> pClass = webAbsorptionPipeline.getClass();            if(!pClass.isAnnotationPresent(SPI.class)){                continue;            }            SPI spi = pClass.getAnnotation(SPI.class);            if(webAbsorptionPipeline.getType() == WebAbsorptionPipeline.DATA_STREAM_PIPELINE){                streamPipelineMap.putIfAbsent(spi.value() , (DataStreamPipeline) webAbsorptionPipeline);            }else {                List<WebAbsorptionPipeline> pipelineList = pipelineMap.get(spi.value());                if(pipelineList == null){                    pipelineList = new LinkedList<>();                    pipelineMap.put(spi.value() , pipelineList);                }                pipelineList.add(webAbsorptionPipeline);            }        }        for (Iterator<WebAbsorptionProcess> iterator = webAbsorptionProcesses.iterator() ; iterator.hasNext() ;){            WebAbsorptionProcess webAbsorptionProcess = iterator.next();            Class<? extends WebAbsorptionProcess> pClass = webAbsorptionProcess.getClass();            if(!pClass.isAnnotationPresent(SPI.class)){                continue;            }            SPI spi = pClass.getAnnotation(SPI.class);            List<WebAbsorptionProcess> processes = processMap.get(spi.value());            if(null == processes){                processes = new LinkedList<>();                processMap.put(spi.value() , processes);            }            processes.add(webAbsorptionProcess);        }    }    public static WebAbsorption newInstance(){        return new WebAbsorption();    }    public WebAbsorption setDataResultItemsSchedule(DataResultItemsSchedule dataResultItemsSchedule){        this.dataResultItemsSchedule = dataResultItemsSchedule;        return this;    }    public WebAbsorption isAsyncPipeline(boolean isAsyncPipeline){        this.isAsyncPipeline = isAsyncPipeline;        return this;    }    public WebAbsorption setThreadNum(int num){        this.threadNum = num;        return this;    }    public WebAbsorption setExecutorService(ExecutorService executorService){        this.executorService = executorService;        return this;    }    public WebAbsorption setID(String id){        this.id = id;        return this;    }    public WebAbsorption setDownloaderPlugin(DownloaderPlugin downloaderPlugin){        this.downloaderPlugin = downloaderPlugin;        return this;    }    public WebAbsorption setSite(Site site){        this.site = site;        return this;    }    public WebAbsorption setDataStreamSchedule(DataStreamSchedule dataStreamSchedule){        this.dataStreamSchedule = dataStreamSchedule;        return this;    }    public WebAbsorption setScheduler(Scheduler scheduler){        this.scheduler = scheduler;        return this;    }    public Spider getSpider(){        return this.spider;    }    public String getSpiderName(){        return this.spiderName;    }    public WebAbsorption builder(String spiderName , String... startUrls) throws IllegalAccessException {        if(site == null){            site = Site.me().setCycleRetryTimes(3).setSleepTime(100).setTimeOut(10000);        }        this.spiderName = spiderName;        List<WebAbsorptionPipeline> pipelines = pipelineMap.get(spiderName);        List<WebAbsorptionProcess> processes = processMap.get(spiderName);        DataStreamPipeline dataStreamPipeline = streamPipelineMap.get(spiderName);        if(pipelines == null || processes == null){            throw new IllegalAccessException("没有定义pipeline 和 process");        }        dataStreamSchedule.setDataStreamPipeline(factory.getClone(dataStreamPipeline));        dataResultItemsSchedule.setWebAbsorptionPipelines(factory.getCloneList(pipelines));        CentralPipeline centralPipeline = factory.getClone(CentralPipeline.class)                .setDataStreamSchedule(dataStreamSchedule)                .setDataResultItemsSchedule(dataResultItemsSchedule)                .setAsync(isAsyncPipeline);        CentralProcessor centralProcessor = factory.getClone(CentralProcessor.class)                .setSite(site)                .setWebAbsorptionProcess(factory.getCloneList(processes));        CentralProxyProvider centralProxyProvider = null;        CentralDownloaderPlugin centralDownloaderPlugin = factory.getClone(CentralDownloaderPlugin.class);        if(null != downloaderPlugin) {            centralProxyProvider = factory.getClone(CentralProxyProvider.class).setDownloaderPlugin(downloaderPlugin);            centralDownloaderPlugin.setDownloaderPlugin(downloaderPlugin);            dataResultItemsSchedule.setDownloaderPlugin(downloaderPlugin);            dataStreamSchedule.setDownloaderPlugin(downloaderPlugin);        }        centralDownloaderPlugin.setProxyProvider(centralProxyProvider);        this.spider = Spider.create(centralProcessor).addPipeline(centralPipeline).setExecutorService(executorService).setDownloader(centralDownloaderPlugin);        if(!StringUtils.isEmpty(this.id)){            spider.setUUID(id);        }        if(threadNum > 0){            spider.thread(threadNum);        }        if(null != scheduler){            spider.setScheduler(scheduler);        }        spider.addUrl(startUrls);        return this;    }    private void start(boolean isAsync) throws IllegalAccessException {        DataStreamPipeline dataStreamPipeline = dataStreamSchedule.getDataStreamPipeline();        if(this.spider == null){            throw new IllegalAccessException("未调用builder方法进行建造");        }        if(null != dataStreamPipeline){            dataStreamSchedule.start();        }        if(null != dataResultItemsSchedule){            if(isAsyncPipeline){                dataResultItemsSchedule.start();            }        }        if(isAsync) {            spider.runAsync();        }else {            spider.run();        }    }    public void run() throws IllegalAccessException {        start( false);    }    public boolean isClose() throws IllegalAccessException {        if(null == spider){            throw new IllegalAccessException("未调用builder方法进行建造");        }        return spider.isExitWhenComplete();    }    public void runAsync() throws IllegalAccessException {        start(true);    }}